---
title: Reinforcement Learning
---


[Axi的强化学习笔记](https://axi404.top/tags/tech%20talk/2)   
[《动手学强化学习》电子书与网课视频材料](https://hrl.boyuai.com/)


# 初探强化学习


## 1 简介

在机器学习领域，有一类重要的任务和人生选择很相似，即**序贯决策（sequential decision making）任务**。决策和预测任务不同，决策往往会带来 “后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。实现序贯决策的机器学习方法就是本文讨论的主题 —**强化学习（reinforcement learning）**。预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变。

## 2 什么是强化学习

广泛地讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。

强化学习用**智能体（agent）** 这个概念来表示做决策的机器。相比于有监督学习中的 “模型”，强化学习中的 “智能体” 强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。

### 智能体与环境的交互方式

智能体和环境之间具体的交互方式如图 1 所示。在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。

![alt text](image-3.png)
（图 1：强化学习中智能体和环境之间的迭代式交互，包含状态 S₁、动作 A₁、奖励 R₁、下一轮状态 S₊₁等元素）

### 智能体的三大关键要素



1.  **感知**：智能体在某种程度上感知环境的状态，从而知道自己所处的现状。

    示例：下围棋的智能体感知当前的棋盘情况；无人车感知周围道路的车辆、行人和红绿灯等情况；机器狗通过摄像头感知面前的图像，通过脚底的力学传感器来感知地面的摩擦功率和倾斜度等情况。

2.  **决策**：智能体根据当前的状态计算出达到目标需要采取的动作的过程。

    示例：针对当前的棋盘决定下一颗落子的位置；针对当前的路况，无人车计算出方向盘的角度和刹车、油门的力度；针对当前收集到的视觉和力觉信号，机器狗给出 4 条腿的齿轮的角速度。

    其中，**策略**是智能体最终体现出的智能形式，是不同智能体之间的核心区别。

3.  **奖励**：环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。

    示例：围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶；机器狗是否在前进而没有摔倒。

    最大化累积奖励期望是智能体提升策略的目标，也是衡量智能体策略好坏的关键指标。

### 强化学习与有监督学习的核心区别



| 对比维度 | 强化学习（面向决策任务）            | 有监督学习（面向预测任务）       |
| ---- | ----------------------- | ------------------- |
| 任务性质 | 多轮交互的序贯决策任务             | 单轮的独立预测任务           |
| 决策考量 | 需考虑长期累积奖励（当前最优动作未必长期最优） | 仅关注单轮预测准确性，无需考虑长期影响 |
| 核心角色 | 智能体（可主动改变环境）            | 模型（仅输出预测信号，不改变环境）   |

## 3 强化学习的环境

从 2 节可知，强化学习的智能体是在和一个**动态环境**的交互中完成序贯决策的。“动态环境” 指环境会随着某些因素的变化而不断演变，在数学和物理中往往用**随机过程**来刻画。生活中几乎所有的系统都属于动态环境，例如一座城市的交通、一片湖中的生态、一场足球比赛、一个星系等。

对于随机过程，其最关键的要素是**状态**以及**状态转移的条件概率分布**。例如，微粒在水中的布朗运动，可由它的起始位置以及下一刻位置相对当前位置的条件概率分布来刻画。

### 环境状态转移的数学刻画

如果在环境自身演变的随机过程中加入智能体的动作（外来干扰因素），那么环境的下一刻状态的概率分布将由当前状态和智能体的动作共同决定，用简单的数学关系表示为：

**下一轮状态 = f (当前状态，智能体的动作)**

这意味着：智能体决策的动作作用到环境中，会使环境发生相应的状态改变，而智能体接下来需要在新的状态下进一步给出决策。

### 环境的随机性来源

与智能体交互的动态环境存在两方面的随机性：



1.  智能体决策动作的随机性（如策略可能是随机策略，不同动作有不同选择概率）；

2.  环境基于当前状态和智能体动作采样下一刻状态的随机性（如无人车行驶中遇到突发的行人横穿马路）。

这种动态随机过程的刻画，凸显了强化学习与有监督学习的本质差异 —— 强化学习是在 “动态变化的数据分布” 中学习，而有监督学习是在 “固定的数据分布” 中学习。

## 4 强化学习的目标

在动态环境下，智能体和环境每次交互时，环境会产生一个**实数标量形式的奖励信号**。这个信号是对当前状态或动作的即时反馈，类似游戏中某一操作的得分；多轮交互的奖励累加后形成**整体回报（return）**，类似一盘游戏的最终得分。

由于环境和策略的随机性，即使初始状态、环境和策略不变，交互结果和回报也可能不同。因此，强化学习中关注**回报的期望**，并将其定义为**价值（value）** —— 这是智能体学习的核心优化目标。

### 价值计算的复杂性

价值的计算需要对 “智能体动作的概率分布” 和 “环境状态转移的概率分布” 做积分运算，这比有监督学习的 “最小化预测误差” 更复杂。但两者的核心逻辑有相似性：均是在某个数据分布下优化一个分数值的期望，只是优化途径完全不同（见 6 节）。

## 5 强化学习中的数据

从数据层面看，强化学习与有监督学习的差异主要体现在**数据的产生方式**和**数据分布的稳定性**上。

### 有监督学习的数据特点



*   数据来源：从给定的**固定数据分布**中采样得到训练数据集；

*   优化逻辑：通过最小化训练数据上的预测误差（如损失函数），寻找模型最优参数；

*   关键假设：训练数据满足**独立同分布**，数据分布全程不变。

### 强化学习的数据特点



*   数据来源：数据是在**智能体与环境的交互过程中实时产生**的；

*   核心特性：若智能体不采取某个动作，该动作对应的数据永远无法观测到 —— 当前训练数据完全依赖之前的决策结果；

*   分布变化：智能体的**策略不同**，与环境交互产生的**数据分布也不同**（如图 2 所示）。

![alt text](image-4.png)
（图 2：强化学习中智能体与环境交互产生相应的数据分布，智能体策略变化会直接导致数据分布变化）

### 关键概念：占用度量（occupancy measure）



*   定义（简要）：归一化的占用度量用于衡量 “智能体决策与动态环境交互过程中，采样到某一具体状态动作对（state-action pair）的概率分布”；

*   核心性质：给定两个策略及其对应的占用度量，**当且仅当占用度量相同时，两个策略相同**。即策略改变必然导致占用度量改变，反之亦然。

### 占用度量的启示



1.  强化学习的核心难点：智能体看到的数据分布会随策略更新而动态变化，无法像有监督学习那样依赖固定数据；

2.  优化目标的转化：策略的价值等价于 “奖励函数在该策略占用度量上的期望”，因此 “寻找最优策略” 可转化为 “寻找最优占用度量”。

## 6 强化学习的独特性

通过前面的分析，我们可通过**优化目标公式**和**范式差异**，清晰对比强化学习与有监督学习的独特性。

### 1. 优化目标公式对比



 | 学习类型  | 优化目标公式                                                                    | 核心含义                               |
| ----- | ------------------------------------------------------------------------- | ---------------------------------- |
| 有监督学习 | 最优模型 = $\arg\min_{\text{模型}} \mathbb{E}_{(\text{特征}, \text{标签}) \sim \mathcal{D}} [\text{损失函数}(\text{标签}, \text{模型}(\text{特征}))]$ | 在固定数据分布𝒟下，最小化损失函数的期望（泛化误差）        |
| 强化学习  | 最优策略 = $\arg\max_{\text{策略}} \mathbb{E}_{(\text{状态}, \text{动作}) \sim \text{策略的占用度量}} [\text{奖励函数}(\text{状态}, \text{动作})]$      | 在策略 π 对应的占用度量 ρ^π 下，最大化奖励函数的期望（价值） |

（注：𝒟表示有监督学习的数据分布，ρ^π 表示强化学习中策略 π 对应的占用度量）

### 2. 核心范式差异



| 对比维度 | 有监督学习                      | 强化学习                              |
| ---- | -------------------------- | --------------------------------- |
| 优化逻辑 | 固定数据分布，修改目标函数（如调整模型参数降低损失） | 固定目标函数（奖励函数），修改数据分布（通过更新策略改变占用度量） |
| 核心关注 | 寻找 “在固定数据上预测准确的模型”         | 寻找 “与环境交互产生最优数据分布的策略”             |
| 本质差异 | 解决 “预测问题”，不改变数据分布          | 解决 “决策问题”，主动改变数据分布                |

## 7 小结

本章通过简短篇幅，介绍了强化学习的基本概念，梳理了其与有监督学习在范式和思维方式上的异同：



1.  **难度差异**：强化学习任务通常更难，因为策略变化会导致数据分布动态改变，且这种改变高度复杂、不可追踪，无法用显式数学公式刻画（类似混沌系统）；

2.  **核心区别**：有监督学习是 “在固定分布中优化预测”，强化学习是 “在动态分布中优化决策”；
